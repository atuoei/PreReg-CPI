{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_model1 = 'DeepChem/esm2_t6_8M_UR50D'\n",
    "esm_model2 = 'DeepChem/esm2_t30_150M_UR50D'\n",
    "class Pro_Feature(nn.Module):\n",
    "   \n",
    "    def __init__(self,pro_model,max_length,finetune=0):\n",
    "        super(Pro_Feature, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.pro_token = AutoTokenizer.from_pretrained(pro_model)\n",
    "        self.pro_model  = AutoModel.from_pretrained(pro_model)\n",
    "\n",
    "        if finetune == 0: \n",
    "            for param in self.pro_model.parameters():\n",
    "                param.requires_grad = False\n",
    "    def forward(self,pro_seq,device):\n",
    "        protein = self.pro_token(pro_seq,\n",
    "                                    truncation=True,\n",
    "                                    padding=True,\n",
    "                                    max_length=self.max_length, \n",
    "                                    add_special_tokens=False)\n",
    "        input_ids=torch.tensor(protein['input_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask=torch.tensor(protein['attention_mask']).unsqueeze(0).to(device)\n",
    "        temp_output=self.pro_model(input_ids=input_ids,attention_mask=attention_mask) \n",
    "        pro_feat = torch.mean(temp_output.last_hidden_state,dim=1) \n",
    "        # pro_feat = temp_output.last_hidden_state \n",
    "        return pro_feat\n",
    "    \n",
    "# all sequence\n",
    "class ProSeq_Feature(nn.Module):\n",
    "    \n",
    "    def __init__(self,pro_model,max_length,finetune=0):\n",
    "        super(ProSeq_Feature, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.pro_token = AutoTokenizer.from_pretrained(pro_model)\n",
    "        self.pro_model  = AutoModel.from_pretrained(pro_model)\n",
    "\n",
    "        if finetune == 0: \n",
    "            for param in self.pro_model.parameters():\n",
    "                param.requires_grad = False\n",
    "    def forward(self,pro_seq,device):\n",
    "        protein = self.pro_token(pro_seq,\n",
    "                                    truncation=True,\n",
    "                                    padding=True,\n",
    "                                    max_length=self.max_length, \n",
    "                                    add_special_tokens=False)\n",
    "        input_ids=torch.tensor(protein['input_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask=torch.tensor(protein['attention_mask']).unsqueeze(0).to(device)\n",
    "        temp_output=self.pro_model(input_ids=input_ids,attention_mask=attention_mask) \n",
    "        \n",
    "        pro_feat = temp_output.last_hidden_state \n",
    "        return pro_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pro = pd.read_csv('reproductive_targets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "model = Pro_Feature(esm_model2,1200).to(device)\n",
    "all_feat = []\n",
    "genes = []\n",
    "for i,row in df_pro.iterrows():\n",
    "    gene = row['gene']\n",
    "    seq = row['seq']\n",
    "    print(gene,len(seq))\n",
    "    \n",
    "    feat = model(seq,device)\n",
    "    all_feat.append(feat)\n",
    "    genes.append(gene)\n",
    "all_feat = torch.cat(all_feat).cpu().numpy()\n",
    "# os.makedirs('Transformer_feats',exist_ok=True)\n",
    "df_feat = pd.DataFrame(all_feat,index=genes,columns = [str(i) for i in range(all_feat.shape[1])])\n",
    "df_feat.to_csv('esm_150m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepChem/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACHE 614\n",
      "ACHE :  torch.Size([614, 640])\n",
      "ACVR1 509\n",
      "ACVR1 :  torch.Size([509, 640])\n",
      "ADRA1A 466\n",
      "ADRA1A :  torch.Size([466, 640])\n",
      "ADRA1B 520\n",
      "ADRA1B :  torch.Size([520, 640])\n",
      "ADRA1D 572\n",
      "ADRA1D :  torch.Size([572, 640])\n",
      "ADRA2A 465\n",
      "ADRA2A :  torch.Size([465, 640])\n",
      "ADRA2B 450\n",
      "ADRA2B :  torch.Size([450, 640])\n",
      "ADRA2C 462\n",
      "ADRA2C :  torch.Size([462, 640])\n",
      "ADRB1 477\n",
      "ADRB1 :  torch.Size([477, 640])\n",
      "ADRB2 413\n",
      "ADRB2 :  torch.Size([413, 640])\n",
      "ADRB3 408\n",
      "ADRB3 :  torch.Size([408, 640])\n",
      "AGTR1 359\n",
      "AGTR1 :  torch.Size([359, 640])\n",
      "AGTR2 363\n",
      "AGTR2 :  torch.Size([363, 640])\n",
      "AKR1C3 323\n",
      "AKR1C3 :  torch.Size([323, 640])\n",
      "AKT1 480\n",
      "AKT1 :  torch.Size([480, 640])\n",
      "ALDH1A1 501\n",
      "ALDH1A1 :  torch.Size([501, 640])\n",
      "ALOX5 674\n",
      "ALOX5 :  torch.Size([674, 640])\n",
      "APAF1 1000\n",
      "APAF1 :  torch.Size([1000, 640])\n",
      "APOBEC3G 384\n",
      "APOBEC3G :  torch.Size([384, 640])\n",
      "AR 920\n",
      "AR :  torch.Size([920, 640])\n",
      "AURKA 403\n",
      "AURKA :  torch.Size([403, 640])\n",
      "AVPR1A 418\n",
      "AVPR1A :  torch.Size([418, 640])\n",
      "BMPR1A 532\n",
      "BMPR1A :  torch.Size([532, 640])\n",
      "BRAF 766\n",
      "BRAF :  torch.Size([766, 640])\n",
      "CACNA1C 1100\n",
      "CACNA1C :  torch.Size([1100, 640])\n",
      "CACNA1H 1200\n",
      "CACNA1H :  torch.Size([1200, 640])\n",
      "CCKAR 428\n",
      "CCKAR :  torch.Size([428, 640])\n",
      "CDC25B 580\n",
      "CDC25B :  torch.Size([580, 640])\n",
      "CDK1 297\n",
      "CDK1 :  torch.Size([297, 640])\n",
      "CDK2 298\n",
      "CDK2 :  torch.Size([298, 640])\n",
      "CFTR 1200\n",
      "CFTR :  torch.Size([1200, 640])\n",
      "CHRM1 460\n",
      "CHRM1 :  torch.Size([460, 640])\n",
      "CTNNB1 781\n",
      "CTNNB1 :  torch.Size([781, 640])\n",
      "CYP17A1 508\n",
      "CYP17A1 :  torch.Size([508, 640])\n",
      "CYP19A1 503\n",
      "CYP19A1 :  torch.Size([503, 640])\n",
      "CYP1B1 543\n",
      "CYP1B1 :  torch.Size([543, 640])\n",
      "CYP2C19 490\n",
      "CYP2C19 :  torch.Size([490, 640])\n",
      "CYP3A4 503\n",
      "CYP3A4 :  torch.Size([503, 640])\n",
      "DRD1 446\n",
      "DRD1 :  torch.Size([446, 640])\n",
      "DRD2 443\n",
      "DRD2 :  torch.Size([443, 640])\n",
      "DRD3 400\n",
      "DRD3 :  torch.Size([400, 640])\n",
      "DRD4 419\n",
      "DRD4 :  torch.Size([419, 640])\n",
      "DRD5 477\n",
      "DRD5 :  torch.Size([477, 640])\n",
      "EDNRB 442\n",
      "EDNRB :  torch.Size([442, 640])\n",
      "EGFR 1000\n",
      "EGFR :  torch.Size([1000, 640])\n",
      "ERBB2 956\n",
      "ERBB2 :  torch.Size([956, 640])\n",
      "ESR1 595\n",
      "ESR1 :  torch.Size([595, 640])\n",
      "ESR2 530\n",
      "ESR2 :  torch.Size([530, 640])\n",
      "ESRRA 423\n",
      "ESRRA :  torch.Size([423, 640])\n",
      "GHSR 366\n",
      "GHSR :  torch.Size([366, 640])\n",
      "GLP1R 463\n",
      "GLP1R :  torch.Size([463, 640])\n",
      "GNRHR 328\n",
      "GNRHR :  torch.Size([328, 640])\n",
      "GSK3B 420\n",
      "GSK3B :  torch.Size([420, 640])\n",
      "HCRTR1 425\n",
      "HCRTR1 :  torch.Size([425, 640])\n",
      "HSD17B1 328\n",
      "HSD17B1 :  torch.Size([328, 640])\n",
      "HSP90AA1 732\n",
      "HSP90AA1 :  torch.Size([732, 640])\n",
      "HTR5A 357\n",
      "HTR5A :  torch.Size([357, 640])\n",
      "IDH1 414\n",
      "IDH1 :  torch.Size([414, 640])\n",
      "ILK 452\n",
      "ILK :  torch.Size([452, 640])\n",
      "INSR 1001\n",
      "INSR :  torch.Size([1001, 640])\n",
      "JAK2 1132\n",
      "JAK2 :  torch.Size([1132, 640])\n",
      "JUN 331\n",
      "JUN :  torch.Size([331, 640])\n",
      "KCNH2 1159\n",
      "KCNH2 :  torch.Size([1159, 640])\n",
      "KISS1R 398\n",
      "KISS1R :  torch.Size([398, 640])\n",
      "MAP2K1 393\n",
      "MAP2K1 :  torch.Size([393, 640])\n",
      "MAP2K2 400\n",
      "MAP2K2 :  torch.Size([400, 640])\n",
      "MAP2K4 399\n",
      "MAP2K4 :  torch.Size([399, 640])\n",
      "MAP2K5 448\n",
      "MAP2K5 :  torch.Size([448, 640])\n",
      "MAPK1 360\n",
      "MAPK1 :  torch.Size([360, 640])\n",
      "MAPK10 464\n",
      "MAPK10 :  torch.Size([464, 640])\n",
      "MAPK11 364\n",
      "MAPK11 :  torch.Size([364, 640])\n",
      "MAPK12 367\n",
      "MAPK12 :  torch.Size([367, 640])\n",
      "MAPK13 365\n",
      "MAPK13 :  torch.Size([365, 640])\n",
      "MAPK14 360\n",
      "MAPK14 :  torch.Size([360, 640])\n",
      "MAPK3 379\n",
      "MAPK3 :  torch.Size([379, 640])\n",
      "MAPK8 427\n",
      "MAPK8 :  torch.Size([427, 640])\n",
      "MAPK9 424\n",
      "MAPK9 :  torch.Size([424, 640])\n",
      "MC3R 323\n",
      "MC3R :  torch.Size([323, 640])\n",
      "MC4R 332\n",
      "MC4R :  torch.Size([332, 640])\n",
      "MC5R 325\n",
      "MC5R :  torch.Size([325, 640])\n",
      "MCHR1 353\n",
      "MCHR1 :  torch.Size([353, 640])\n",
      "MLNR 412\n",
      "MLNR :  torch.Size([412, 640])\n",
      "MMP14 582\n",
      "MMP14 :  torch.Size([582, 640])\n",
      "MMP2 660\n",
      "MMP2 :  torch.Size([660, 640])\n",
      "MMP9 707\n",
      "MMP9 :  torch.Size([707, 640])\n",
      "MYC 454\n",
      "MYC :  torch.Size([454, 640])\n",
      "MYLK 1134\n",
      "MYLK :  torch.Size([1134, 640])\n",
      "NCOA1 1100\n",
      "NCOA1 :  torch.Size([1100, 640])\n",
      "NCOA3 1100\n",
      "NCOA3 :  torch.Size([1100, 640])\n",
      "NFE2L2 605\n",
      "NFE2L2 :  torch.Size([605, 640])\n",
      "NFKB1 968\n",
      "NFKB1 :  torch.Size([968, 640])\n",
      "NOS1 1001\n",
      "NOS1 :  torch.Size([1001, 640])\n",
      "NOS2 1153\n",
      "NOS2 :  torch.Size([1153, 640])\n",
      "NR1H4 486\n",
      "NR1H4 :  torch.Size([486, 640])\n",
      "NR1I2 434\n",
      "NR1I2 :  torch.Size([434, 640])\n",
      "NR1I3 352\n",
      "NR1I3 :  torch.Size([352, 640])\n",
      "NR3C1 777\n",
      "NR3C1 :  torch.Size([777, 640])\n",
      "NSD2 912\n",
      "NSD2 :  torch.Size([912, 640])\n",
      "OPRD1 372\n",
      "OPRD1 :  torch.Size([372, 640])\n",
      "OPRK1 380\n",
      "OPRK1 :  torch.Size([380, 640])\n",
      "OPRM1 400\n",
      "OPRM1 :  torch.Size([400, 640])\n",
      "OXTR 389\n",
      "OXTR :  torch.Size([389, 640])\n",
      "PGR 933\n",
      "PGR :  torch.Size([933, 640])\n",
      "PKMYT1 499\n",
      "PKMYT1 :  torch.Size([499, 640])\n",
      "PLK1 603\n",
      "PLK1 :  torch.Size([603, 640])\n",
      "PPARD 441\n",
      "PPARD :  torch.Size([441, 640])\n",
      "PPARG 505\n",
      "PPARG :  torch.Size([505, 640])\n",
      "PPP1CA 330\n",
      "PPP1CA :  torch.Size([330, 640])\n",
      "PRKAA1 559\n",
      "PRKAA1 :  torch.Size([559, 640])\n",
      "PTGS2 604\n",
      "PTGS2 :  torch.Size([604, 640])\n",
      "RAF1 648\n",
      "RAF1 :  torch.Size([648, 640])\n",
      "RAPGEF3 923\n",
      "RAPGEF3 :  torch.Size([923, 640])\n",
      "RAPGEF4 1011\n",
      "RAPGEF4 :  torch.Size([1011, 640])\n",
      "RARA 462\n",
      "RARA :  torch.Size([462, 640])\n",
      "RELA 551\n",
      "RELA :  torch.Size([551, 640])\n",
      "ROCK2 922\n",
      "ROCK2 :  torch.Size([922, 640])\n",
      "RORC 518\n",
      "RORC :  torch.Size([518, 640])\n",
      "RPS6KA1 735\n",
      "RPS6KA1 :  torch.Size([735, 640])\n",
      "RXRA 462\n",
      "RXRA :  torch.Size([462, 640])\n",
      "SELE 610\n",
      "SELE :  torch.Size([610, 640])\n",
      "SENP6 1112\n",
      "SENP6 :  torch.Size([1112, 640])\n",
      "SENP7 1050\n",
      "SENP7 :  torch.Size([1050, 640])\n",
      "SENP8 212\n",
      "SENP8 :  torch.Size([212, 640])\n",
      "SLC6A3 620\n",
      "SLC6A3 :  torch.Size([620, 640])\n",
      "SLK 1100\n",
      "SLK :  torch.Size([1100, 640])\n",
      "SRC 536\n",
      "SRC :  torch.Size([536, 640])\n",
      "STAT1 750\n",
      "STAT1 :  torch.Size([750, 640])\n",
      "STAT3 770\n",
      "STAT3 :  torch.Size([770, 640])\n",
      "STK10 968\n",
      "STK10 :  torch.Size([968, 640])\n",
      "TDP1 608\n",
      "TDP1 :  torch.Size([608, 640])\n",
      "TGFBR1 503\n",
      "TGFBR1 :  torch.Size([503, 640])\n",
      "TGFBR2 567\n",
      "TGFBR2 :  torch.Size([567, 640])\n",
      "THRB 461\n",
      "THRB :  torch.Size([461, 640])\n",
      "TP53 393\n",
      "TP53 :  torch.Size([393, 640])\n",
      "TRPC5 973\n",
      "TRPC5 :  torch.Size([973, 640])\n",
      "VDR 427\n",
      "VDR :  torch.Size([427, 640])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "model = ProSeq_Feature(esm_model2,1200).to(device)\n",
    "\n",
    "for i,row in df_pro.iterrows():\n",
    "    gene = row['gene']\n",
    "    seq = row['seq']\n",
    "    # if len(seq)>1000:\n",
    "    \n",
    "    # if os.path.exists(f'proallfeat/{gene}.pt'):\n",
    "    #     continue\n",
    "    print(gene,len(seq))\n",
    "    feat = model(seq,device).squeeze(0)\n",
    "    print(gene,': ',feat.shape)\n",
    "    savefeat = feat\n",
    "    torch.save(savefeat,f'proallfeat/{gene}.pt')\n",
    "\n",
    "    del feat\n",
    "    del savefeat\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
